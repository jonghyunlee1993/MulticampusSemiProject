mutate(prob = ifelse(d == "versicolor", 1, 0)) %>%
ggplot(aes(Sepal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabete-pos"
)
library(dplyr)
d %>%
mutate(prob = ifelse(d == "versicolor", 1, 0)) %>%
ggplot(aes(Sepal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabete-pos"
)
d
# logisitc은 2개의 변수를 취하기 때문에 subset 함수로 조건 추출
# subset은 행 단위 추출
d = subset(data, Species %in% c("virginica", "versicolor"))
d
d %>%
mutate(prob = ifelse(d == "versicolor", 1, 0)) %>%
ggplot(aes(Sepal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabete-pos"
)
d %>%
mutate(prob = ifelse(d$Species == "versicolor", 1, 0)) %>%
ggplot(aes(Sepal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabete-pos"
)
d %>%
mutate(prob = ifelse(d$Species == "versicolor", 1, 0)) %>%
ggplot(aes(Sepal.Width, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Sepal.Length",
y = "Probability of being versicolor"
)
d %>%
mutate(prob = ifelse(d$Species == "versicolor", 1, 0)) %>%
ggplot(aes(Petal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Sepal.Length",
y = "Probability of being versicolor"
)
# logistic prediction
# Split the data into training and test set
set.seed(123)
training.samples = d$Species %>%
createDataPartition(p = 0.8, list = FALSE)
# logistic prediction
library(mlbench)
# Split the data into training and test set
set.seed(123)
training.samples = d$Species %>%
createDataPartition(p = 0.8, list = FALSE)
library(caret)
require("caret")
require(caret)
install.packages("caret")
library(caret)
training.samples = d$Species %>%
createDataPartition(p = 0.8, list = FALSE)
training.samples
train.data  <- d[training.samples, ]
test.data <- d[-training.samples, ]
test.data
train.data
model_predict = glm(Species ~ ., family = 'binomial', data = train.data)
pred = model %>% predict(test.data, type = "response")
pred
predicted.classes <- ifelse(pred > 0.5, "verginica", "vesicolor")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
test.data$Species
predicted.classes <- ifelse(pred > 0.5, "verginica", "versicolor")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
predicted.classes <- ifelse(pred > 0.5, "versicolor", "verginica")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
predicted.classes <- ifelse(pred > 0.5, "verginica", "versicolor")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
# Split the data into training and test set
set.seed(133)
training.samples = d$Species %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- d[training.samples, ]
test.data <- d[-training.samples, ]
d$Species
training.samples = d$Species %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- d[training.samples, ]
test.data <- d[-training.samples, ]
test.data
model_predict = glm(Species ~ ., family = 'binomial', data = train.data)
pred = model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(pred > 0.5, "versicolor", "virginica")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
predicted.classes <- ifelse(pred > 0.5, "virginica", "versicolor")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
f = fitted(model)
ifelse(f > .5, 1, 0) == as.numeric(d$Species)
f
ifelse(f > .5, 1, 0)
ifelse(f > .5, 1, 0) == as.numeric(d$Species)-1
as.numeric(d$Species)-1
as.numeric(d$Species)-2
ifelse(f > .5, 1, 0) == as.numeric(d$Species)-2
is_correct = ifelse(f > .5, 1, 0) == as.numeric(d$Species)-2
mean(is_correct)
# 예측 값 확인
round(fitted(model)[c(1:5, 51:55)])
mean(predicted.classes == test.data$Species)
is_correct = ifelse(f > .5, 1, 0) == as.numeric(d$Species)-1
mean(is_correct)
is_correct = ifelse(f > .5, 1, 0) == as.numeric(d$Species)-2
mean(is_correct)
ifelse(f > .5, 1, 0)
as.numeric(d$Species)
as.numeric(d$Species)-2
is_correct = (ifelse(f > .5, 1, 0)) == (as.numeric(d$Species)-1)
mean(is_correct)
is_correct = ifelse(f > .5, 1, 0) == as.numeric(d$Species)-2
mean(is_correct)
d %>%
mutate(prob = ifelse(d$Species == "virginica", 1, 0)) %>%
ggplot(aes(Petal.Length, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Petal.Length",
y = "Probability of being versicolor"
)
unique(d$Species)
unique(as.numeric(d$Species))
as.numeric(d$Species)
as.numeric(d$Species)-2
model
unique(data$Species)
predicted.classes <- ifelse(pred > 0.5, "virginica", "versicolor")
head(predicted.classes)
mean(predicted.classes == test.data$Species)
?xtabs
rm(list=ls())
library(ggplot2)
library(reshape2)
library(ggplot2)
library(reshape2)
library(foreign)
library(nnet)
m1 = read.dta("https://stats.idre.ucla.edu/stat/data/hsbdeom.dta")
m1 = read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
View(m1)
str(m1)
levels(m1$honors)
glm(ses ~ read + write + math + science + socst, data = m1, family = "binomial")
model = glm(ses ~ read + write + math + science + socst, data = m1, family = "binomial")
summary(model)
model = glm(ses ~ female + schtyp + read + write + math + science + socst, data = m1, family = "binomial")
model2 = glm(ses ~ female + schtyp + read + write + math + science + socst, data = m1, family = "binomial")
summary(model2)
require("randomForest")
install.packages("randomForest")
library(randomForest)
rf = randomForest(Speices ~ ., data = iris)
data = iris
rf = randomForest(Species ~ ., data = iris)
summary(rf)
plot(rf)
barplot(rf)
rf
rf$importance
barh(rf$importance)
barplot(rf$importance)
plot(rf$importance)
rf$importance
temp = rf$importance
View(temp)
rownames(temp)
barplot(temp, rownames(temp))
temp
library(ggplot2)
df = data.frame(name = rownames(rf$importance),
value = rf$importance)
df
ggplot(data = temp, aes(name, MeanDecreaseGini)) +
geom_bar(stat = "identity")
rf$importance
rf$importance[1]
rf$importance[1:4]
df = data.frame(name = rownames(rf$importance),
value = rf$importance[1:4])
df
ggplot(data = temp, aes(name, MeanDecreaseGini)) +
geom_bar(stat = "identity")
ggplot(data = df, aes(name, MeanDecreaseGini)) +
geom_bar(stat = "identity")
ggplot(data = df, aes(name, value)) +
geom_bar(stat = "identity")
sort(df$value)
library(dplyr)
df = df %>% sort(desc(value))
df
rf2 = randomForest(Species ~ ., data = iris, importance = T)
rf2
importance(rf2)
varImpPlot(rf2)
varImpPlot(rf2, main = "varImpPlot of Iris")
?expand.grid
install.packages('expand.grid')
install.packages("rmarkdown")
install.packages("knitr")
setwd("C://R_study_secret/data")
rm(list = ls())
movie = read.csv("movie.csv", header = T)
library(e1071)
nm = naiveBayes(movie[1:5], movie$장르, laplace = 0)
summary(nm)
nm
result = predict(nm, movie[1:5])
sum(movie$장르 != result)
?naiveBayes
library(caret)
rm(list = ls())
library(caret)
idx = createDataPartition(iris$Species, p = 0.7, list = F)
iris_train = iris[idx, ]
iris_train = iris[-idx, ]
library(nnet)
iris_train_sacle = as.data.frame(sapply(iris_train[, -5]), scale))
iris_train_sacle = as.data.frame(sapply(iris_train[, -5], scale)))
sapply(iris_train[, -5], scale)
iris_train_sacle = as.data.frame(sapply(iris_train[, -5], scale))
iris_test_scale = as.data.frame(sapply(iris_test[, -5], sacle))
iris_test_scale = as.data.frame(sapply(iris_test[, -5], scale))
iris_train = iris[idx, ]
iris_train = iris[-idx, ]
iris_train_sacle = as.data.frame(sapply(iris_train[, -5], scale))
iris_test_scale = as.data.frame(sapply(iris_test[, -5], scale))
iris_test = iris[-idx, ]
iris_test_scale = as.data.frame(sapply(iris_test[, -5], scale))
iris_train_sacle = as.data.frame(sapply(iris_train[, -5], scale))
iris_test_scale = as.data.frame(sapply(iris_test[, -5], scale))
iris_train_sacle$Species = iris_train$Species
iris_test_sacle$Species = iris_test$Species
rm(list = ls())
library(caret)
idx = createDataPartition(iris$Species, p = 0.7, list = F)
iris_train = iris[idx, ]
iris_test = iris[-idx, ]
library(nnet)
iris_train_scale = as.data.frame(sapply(iris_train[, -5], scale))
iris_test_scale = as.data.frame(sapply(iris_test[, -5], scale))
iris_train_scale$Species = iris_train$Species
iris_test_scale$Species = iris_test$Species
nnet_res = nnet(Species ~ ., iris_train_scale, size = 3)
rm(nnet_res)
nnet_model = nnet(Species ~ ., iris_train_scale, size = 3)
nnet_pred = predict(nnet_model, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, nnet_pred)
nnet_pred
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 10)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 25)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 50)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 2)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 4)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 5
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 10)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 11)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 9)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
nnet_model2 = nnet(Species ~ ., iris_train_scale, size = 10)
nnet_pred2 = predict(nnet_model2, iris_test_scale, type = "class")
confusionMatrix(iris_test_scale$Species, as.factor(nnet_pred2))
View(iris_train_scale)
getwd()
##
prob = read.csv("problem.csv", header = T, stringsAsFactors = F)
head(prob)
for (i in 1:30){
print(prob[i])
print(prob[i] * 1/5)
}
temp = iris_train[, -5]
View(temp)
temp2 = sapply(temp$Sepal.Length, scale)
temp3 = cbind(temp$Sepal.Length, temp2)
View(temp3)
View(temp)
View(sapply(temp, scale))
View(sapply(temp[[1]], scale))
View(sapply(temp[1], scale))
for (i in 1:2){
print(prob[i])
print(prob[i] * 1/5)
}
View(prob)
prob[i]
for (i in 1:30){
prob[i] = prob[i] * 1/5
}
?with
prob$accident2 = with(prob, ifelse(accident == "suicide" | accident == "violence",
1, 0))
library(nnet)
prob = prob[-31]
model = nnet(accident2 ~ ., data = prob, size = 10)
pred = predict(model, prob)
pred
prob$accident2
prob
pred
prob
nnet(accident2 ~ ., data = prob, size = 10)
model = nnet(accident2 ~ ., data = prob, size = 10)
summary(model)
plot(model)
cbind(prob$accident2, pred > 0.5)
##
prob = read.csv("problem.csv", header = T, stringsAsFactors = F)
head(prob)
for (i in 1:30){
prob[i] = prob[i] * 1/5
}
prob$accident2 = with(prob, ifelse(accident == "suicide" | accident == "violence",
1, 0))
library(nnet)
prob = prob[-31]
model = nnet(accident2 ~ ., data = prob, size = 10)
pred = predict(model, prob)
cbind(prob$accident2, pred > 0.5)
sum(as.numeric(pred > 0.5) != prob$accident2)
model = nnet(accident2 ~ ., data = prob, size = 20)
pred = predict(model, prob)
cbind(prob$accident2, pred > 0.5)
sum(as.numeric(pred > 0.5) != prob$accident2)
##
library(neuralnet)
install.packages("neuralnet")
##
library(neuralnet)
as.formula(paste("accident2 ~ ", paste0(xnam, collapse = "+")))
xnam = paste0("ans", 1:30)
as.formula(paste("accident2 ~ ", paste0(xnam, collapse = "+")))
model2 = neuralnet(fmal, data = prob, hidden = 20)
fmla = as.formula(paste("accident2 ~ ", paste0(xnam, collapse = "+")))
model2 = neuralnet(fmla, data = prob, hidden = 20)
plot(model2)
install.packages("repmis")
?source_data
library(rempis)
library(repmis)
source_data("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_liner_model.Rdata")
library(httr)
df <-read.csv(text=GET("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv"), header=T)
df <-read.csv(text = GET("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv"), header=T, fileEncoding = "UTF-8")
df <-read.csv(text = GET("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv"), header=T, fileEncoding = "euc-kr")
df <-read.csv("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv", header=T, fileEncoding = "euc-kr")
View(df)
library(RCurl)
x <- getURL("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv")
y <- read.csv(text = x)
x <- "https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv"
y <- read.csv(text = x)
library(curl)
x <- curl("https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv")
y <- read.csv(x)
View(y)
x <- "https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.csv"
load(url(x))
x <- "https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_linear_model.Rdata"
load(url(x))
x <- "https://github.com/jonghyunlee1993/Multicampus_semi/blob/master/JH_working/proc/final_df_for_liner_model.Rdata"
load(url(x))
download.file(x, "myfile")
load("myfile")
getwd()
download.file(x, "myfile.Rdata")
load("myfile")
dir()
load("myfile.Rdata")
url = "https://github.com/jonghyunlee1993/Multicampus_semi/raw/master/JH_working/proc/final_df_for_liner_model.Rdata"
test = read.csv(url)
url = "https://raw.githubusercontent.com/jonghyunlee1993/Multicampus_semi/master/JH_working/proc/final_df_for_linear_model.csv"
read.csv(url)
read.csv(url, fileEncoding = "UTF-8")
View(df)
url = "https://raw.githubusercontent.com/jonghyunlee1993/Multicampus_semi/master/JH_working/proc/final_df_for_linear_model.csv"
df = read.csv(url, fileEncoding = "UTF-8")
View(df)
knitr::include_graphics("departments.jpeg")
구분 | 설명 | 변수명
----- |------ | -----
백화점 요소 | 백화점 총 면적(m²) | size
지역 환경 요소 | 지하철역 하차 인원(명) | arrival
지역 환경 요소 | 주거지 비율(%) | residential_area
지역 환경 요소 | 상업지 비율(%) | commercial_area
지역 환경 요소 | 녹지 비율(%) | green_area
지역 환경 요소 | 인구 밀도(명/㎢) | pop_density
기상 조건 요소 | 일평균 미세먼지(㎍/m³) | fine_dust
기상 조건 요소 | 일평균 초미세먼지(㎍/m³) | hyper_dust
기상 조건 요소 | 일평균 강수량(mm) | mean_precipitation
기상 조건 요소 | 일평균 기온(°C) | mean_temperature
기상 조건 요소 | 일평균 풍속(m/s) | mean_wind
기상 조건 요소 | 일평균 적설량(cm) | mean_snow
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
ggplot(data = df, aes(x = mean_pop)) +
geom_histogram()
hist1 = ggplot(data = df, aes(x = mean_pop)) +
geom_histogram() +
ggtitle("log 변환 전 일평균 유동 인구")
hist1 = ggplot(data = df, aes(x = mean_pop)) +
geom_histogram() +
ggtitle("log 변환 전 일평균 유동 인구")
hist2 = ggplot(data = df, aes(x = log(mean_pop))) +
geom_histogram() +
ggtitle("log 변환 후 일평균 유동 인구")
grid.arrange(hist1, hist2, ncol = 2)
library(tidyverse)
library(gridExtra)
library(grid)
library(png)
library(downloader)
library(grDevices)
install.packages("downloader")
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
library(gridExtra)
library(grid)
plot(model_step)
model_step = lm(formula = log(mean_pop) ~ size + residential_area + commercial_area +
green_area + pop_density + arrival + fine_dust + hyper_dust,
data = df)
summary(model_step)
plot(model_step)
resid_plot = plot(model_step)
print(resid_plot[2])
qqnorm(model_step)
model_stdres = rstandard(model_step)
qqnorm(model_stdres, ylab = "Standardized Residuals", xlab = "Normal Scores",
main = "QQ plot of Linear Model")
qqline(model_stdres)
qqnorm(model_stdres, ylab = "Standardized Residuals", xlab = "Normal Scores",
main = "QQ plot of Linear Model")
qqline(model_stdres)
qqplot(model_stdres, ylab = "Standardized Residuals", xlab = "Normal Scores",
main = "QQ plot of Linear Model")
qqnorm(model_stdres, ylab = "Standardized Residuals", xlab = "Normal Scores",
main = "QQ plot of Linear Model")
qqline(model_stdres)
ggplot(data=as.data.frame(qqnorm( rstandard(model_step) , plot=F)), mapping=aes(x=x, y=y)) +
geom_point() + geom_smooth(method="lm", se=FALSE)
model_stdres = rstandard(model_step)
ggplot(data=as.data.frame(qqnorm( model_stdres , plot=F)), mapping=aes(x=x, y=y)) +
geom_point() + geom_smooth(method="lm", se=FALSE) +
labs(x = "Standardized Residuals", y= "Normal Scores") +
ggtitle("QQ plot of Linear Model")
knit("RGitHub.Rmd")
?knit
getwd()
setwd("C:/Multicampus_semi/JH_working/project_report")
knit("Project_Report.Rmd")
